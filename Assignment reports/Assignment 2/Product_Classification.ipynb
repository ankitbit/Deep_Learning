{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Product Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitbit/Deep_Learning/blob/master/Assignment%20reports/Assignment%202/Product_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Mg8XMvKaBDHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNyghpWVNGn4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install gensim\n",
        "! pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VIElG7ChLTt9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing major libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 30\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin\"\n",
        "category_index = { 'LEGO':0, \n",
        "                   'Disney':1, \n",
        "                   'Oxford Diecast':2,\n",
        "                   'Playmobil':3,\n",
        "                   'Star Wars':4,\n",
        "                   'The Puppet Company':5,\n",
        "                   'Hasbro':6,\n",
        "                   'Mattel':7}\n",
        "category_reverse_index = dict((y,x) for (x,y) in category_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSM-2MS83r49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "34649f88-38c4-4f67-8d95-bf262a8ed1cd"
      },
      "cell_type": "code",
      "source": [
        "category_reverse_index"
      ],
      "execution_count": 357,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'LEGO',\n",
              " 1: 'Disney',\n",
              " 2: 'Oxford Diecast',\n",
              " 3: 'Playmobil',\n",
              " 4: 'Star Wars',\n",
              " 5: 'The Puppet Company',\n",
              " 6: 'Hasbro',\n",
              " 7: 'Mattel'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 357
        }
      ]
    },
    {
      "metadata": {
        "id": "iUNJxKW9Y_Og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cd117989-0298-4f77-bac2-ad097950b4aa"
      },
      "cell_type": "code",
      "source": [
        "! wget 'https://raw.githubusercontent.com/ankitbit/Deep_Learning/master/Assignment%20Reports/Assignment%202/data/amazon_co-ecommerce_sample.csv'\n",
        "datasets = pd.read_csv('amazon_co-ecommerce_sample.csv', sep=',')\n",
        "datasets = datasets[['description', 'manufacturer']]"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-13 21:03:22--  https://raw.githubusercontent.com/ankitbit/Deep_Learning/master/Assignment%20Reports/Assignment%202/data/amazon_co-ecommerce_sample.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35284814 (34M) [text/plain]\n",
            "Saving to: ‘amazon_co-ecommerce_sample.csv.6’\n",
            "\n",
            "amazon_co-ecommerce 100%[===================>]  33.65M  43.8MB/s    in 0.8s    \n",
            "\n",
            "2018-11-13 21:03:24 (43.8 MB/s) - ‘amazon_co-ecommerce_sample.csv.6’ saved [35284814/35284814]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bu6VPKaoaf6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4afffa28-df3d-4e11-b163-b3b0be57df7b"
      },
      "cell_type": "code",
      "source": [
        "# Examination of null values\n",
        "print(\"Make sure there are no null values in the datasets\")\n",
        "print(\"Has null values: \", datasets.isnull().values.any())\n",
        "\n",
        "datasets = datasets.dropna()\n",
        "print(\"Has null values: \", datasets.isnull().values.any())"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Make sure there are no null values in the datasets\n",
            "Has null values:  True\n",
            "Has null values:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TJE4paoWchuU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "58978f62-1a2d-4162-f0a4-d5a2514dd1b3"
      },
      "cell_type": "code",
      "source": [
        "datasets.head(6)"
      ],
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>manufacturer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Product Description Hornby 2014 Catalogue Box ...</td>\n",
              "      <td>Hornby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Size Name:Large FunkyBuys® Large Christmas Hol...</td>\n",
              "      <td>FunkyBuys</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BIG CLASSIC TOY TRAIN SET TRACK CARRIAGE LIGHT...</td>\n",
              "      <td>ccf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hornby 00 Gauge BR Hawksworth 3rd Class W 2107...</td>\n",
              "      <td>Hornby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Product Description Hornby RailRoad 0-4-0 Gild...</td>\n",
              "      <td>Hornby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>These delicate model garden lights are mainly ...</td>\n",
              "      <td>Generic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         description manufacturer\n",
              "0  Product Description Hornby 2014 Catalogue Box ...       Hornby\n",
              "1  Size Name:Large FunkyBuys® Large Christmas Hol...    FunkyBuys\n",
              "2  BIG CLASSIC TOY TRAIN SET TRACK CARRIAGE LIGHT...          ccf\n",
              "3  Hornby 00 Gauge BR Hawksworth 3rd Class W 2107...       Hornby\n",
              "4  Product Description Hornby RailRoad 0-4-0 Gild...       Hornby\n",
              "5  These delicate model garden lights are mainly ...      Generic"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 360
        }
      ]
    },
    {
      "metadata": {
        "id": "TaMpSCqfa4zJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "grouped = datasets.groupby('manufacturer').count().reset_index()\n",
        "grouped = grouped.sort_values(['description'], ascending=False)\n",
        "category_names= [val for val in grouped['manufacturer'][0:8]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NcHod9Jp01_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "for name in category_names:\n",
        "  data = data.append(datasets[datasets['manufacturer']==name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHhyPpAm1xTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrCf6DH_e_uy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = data\n",
        "category = data['manufacturer']\n",
        "data = data['description']\n",
        "\n",
        "def preprocess(text):\n",
        "    text= text.strip().lower().split()\n",
        "    return \" \".join(text)\n",
        "\n",
        "datasets['description'] = datasets['description'].apply(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5PoKLmvdofGN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "category = pd.get_dummies(list(category))\n",
        "category = np.array(category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsJreU6xu35P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2078ecce-5fad-43b3-ee46-c8b8a7466758"
      },
      "cell_type": "code",
      "source": [
        "category.shape"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1046, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 325
        }
      ]
    },
    {
      "metadata": {
        "id": "6pc8vgIt2ezF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e53f428-4cfb-45a4-91d2-7f8f8020d276"
      },
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1046,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        }
      ]
    },
    {
      "metadata": {
        "id": "D0V-Co7hgnbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(datasets['description'])\n",
        "\n",
        "datasets_sequences = tokenizer.texts_to_sequences(data)\n",
        "padded_data = pad_sequences(datasets_sequences, maxlen= MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PdfvDMTF2ROc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-RLKCANsksN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_data, category, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1xJhV6kiunU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "10fxu72Lm_lL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUw13fiYfRHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "548fa61b-dee4-46d0-c767-d279a99d8877"
      },
      "cell_type": "code",
      "source": [
        "! wget 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-13 16:46:48--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.0.35\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.0.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  26.0MB/s    in 26s     \n",
            "\n",
            "2018-11-13 16:47:14 (61.3 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TH6Dt2VdgQL_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SC0nvSCMPO97",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQLBCS2eeACQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04707456-819a-4e98-f781-6de6f77449d5"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embedding_matrix[i] = word2vec.word_vec(word)\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
        "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Null word embeddings: 13201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e2P0tz_F77MG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCjOvQCpT4JW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "9f1ec373-3479-4aad-f8ff-fa74dd4f6966"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(embedding_layer)\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(8,activation='sigmoid'))\n",
        "model2.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "model2.summary()"
      ],
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 30, 300)           9858600   \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 9000)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 8)                 72008     \n",
            "=================================================================\n",
            "Total params: 9,930,608\n",
            "Trainable params: 72,008\n",
            "Non-trainable params: 9,858,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IkNUNno6UIcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1871
        },
        "outputId": "d2fb7aa5-99fa-4e87-a6bc-cc7748b692b8"
      },
      "cell_type": "code",
      "source": [
        "history_model = model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=128)\n",
        "score = model2.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 784 samples, validate on 262 samples\n",
            "Epoch 1/50\n",
            "784/784 [==============================] - 1s 740us/step - loss: 1.9388 - acc: 0.3584 - val_loss: 1.7573 - val_acc: 0.4771\n",
            "Epoch 2/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 1.5460 - acc: 0.7041 - val_loss: 1.5976 - val_acc: 0.5382\n",
            "Epoch 3/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 1.2845 - acc: 0.8176 - val_loss: 1.4881 - val_acc: 0.5725\n",
            "Epoch 4/50\n",
            "784/784 [==============================] - 0s 48us/step - loss: 1.0845 - acc: 0.8992 - val_loss: 1.4084 - val_acc: 0.5916\n",
            "Epoch 5/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.9289 - acc: 0.9222 - val_loss: 1.3433 - val_acc: 0.5802\n",
            "Epoch 6/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.8065 - acc: 0.9349 - val_loss: 1.3014 - val_acc: 0.6107\n",
            "Epoch 7/50\n",
            "784/784 [==============================] - 0s 56us/step - loss: 0.7041 - acc: 0.9528 - val_loss: 1.2727 - val_acc: 0.6107\n",
            "Epoch 8/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.6186 - acc: 0.9541 - val_loss: 1.2396 - val_acc: 0.6221\n",
            "Epoch 9/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.5458 - acc: 0.9630 - val_loss: 1.2197 - val_acc: 0.6260\n",
            "Epoch 10/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.4844 - acc: 0.9694 - val_loss: 1.2016 - val_acc: 0.6183\n",
            "Epoch 11/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.4307 - acc: 0.9707 - val_loss: 1.1738 - val_acc: 0.6336\n",
            "Epoch 12/50\n",
            "784/784 [==============================] - 0s 47us/step - loss: 0.3843 - acc: 0.9732 - val_loss: 1.1627 - val_acc: 0.6183\n",
            "Epoch 13/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.3443 - acc: 0.9821 - val_loss: 1.1478 - val_acc: 0.6221\n",
            "Epoch 14/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.3103 - acc: 0.9821 - val_loss: 1.1366 - val_acc: 0.6298\n",
            "Epoch 15/50\n",
            "784/784 [==============================] - 0s 56us/step - loss: 0.2804 - acc: 0.9885 - val_loss: 1.1256 - val_acc: 0.6221\n",
            "Epoch 16/50\n",
            "784/784 [==============================] - 0s 52us/step - loss: 0.2542 - acc: 0.9872 - val_loss: 1.1199 - val_acc: 0.6298\n",
            "Epoch 17/50\n",
            "784/784 [==============================] - 0s 48us/step - loss: 0.2303 - acc: 0.9885 - val_loss: 1.1180 - val_acc: 0.6221\n",
            "Epoch 18/50\n",
            "784/784 [==============================] - 0s 46us/step - loss: 0.2102 - acc: 0.9872 - val_loss: 1.1400 - val_acc: 0.6069\n",
            "Epoch 19/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.1968 - acc: 0.9847 - val_loss: 1.1047 - val_acc: 0.6183\n",
            "Epoch 20/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.1739 - acc: 0.9898 - val_loss: 1.1042 - val_acc: 0.6221\n",
            "Epoch 21/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.1604 - acc: 0.9885 - val_loss: 1.1008 - val_acc: 0.6183\n",
            "Epoch 22/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.1469 - acc: 0.9898 - val_loss: 1.0918 - val_acc: 0.6183\n",
            "Epoch 23/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.1367 - acc: 0.9898 - val_loss: 1.0923 - val_acc: 0.6336\n",
            "Epoch 24/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.1251 - acc: 0.9923 - val_loss: 1.0961 - val_acc: 0.6298\n",
            "Epoch 25/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.1160 - acc: 0.9911 - val_loss: 1.0919 - val_acc: 0.6221\n",
            "Epoch 26/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.1055 - acc: 0.9923 - val_loss: 1.0918 - val_acc: 0.6260\n",
            "Epoch 27/50\n",
            "784/784 [==============================] - 0s 52us/step - loss: 0.1016 - acc: 0.9911 - val_loss: 1.0939 - val_acc: 0.6221\n",
            "Epoch 28/50\n",
            "784/784 [==============================] - 0s 52us/step - loss: 0.0925 - acc: 0.9936 - val_loss: 1.0989 - val_acc: 0.6183\n",
            "Epoch 29/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.0854 - acc: 0.9936 - val_loss: 1.1061 - val_acc: 0.6221\n",
            "Epoch 30/50\n",
            "784/784 [==============================] - 0s 48us/step - loss: 0.0810 - acc: 0.9911 - val_loss: 1.1018 - val_acc: 0.6260\n",
            "Epoch 31/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.0768 - acc: 0.9911 - val_loss: 1.1054 - val_acc: 0.6221\n",
            "Epoch 32/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.0707 - acc: 0.9923 - val_loss: 1.1098 - val_acc: 0.6298\n",
            "Epoch 33/50\n",
            "784/784 [==============================] - 0s 52us/step - loss: 0.0657 - acc: 0.9885 - val_loss: 1.1140 - val_acc: 0.6221\n",
            "Epoch 34/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.0632 - acc: 0.9911 - val_loss: 1.1133 - val_acc: 0.6221\n",
            "Epoch 35/50\n",
            "784/784 [==============================] - 0s 47us/step - loss: 0.0608 - acc: 0.9923 - val_loss: 1.1183 - val_acc: 0.6221\n",
            "Epoch 36/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.0567 - acc: 0.9923 - val_loss: 1.1256 - val_acc: 0.6298\n",
            "Epoch 37/50\n",
            "784/784 [==============================] - 0s 56us/step - loss: 0.0535 - acc: 0.9911 - val_loss: 1.1336 - val_acc: 0.6107\n",
            "Epoch 38/50\n",
            "784/784 [==============================] - 0s 52us/step - loss: 0.0521 - acc: 0.9911 - val_loss: 1.1384 - val_acc: 0.6183\n",
            "Epoch 39/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.0491 - acc: 0.9911 - val_loss: 1.1402 - val_acc: 0.6145\n",
            "Epoch 40/50\n",
            "784/784 [==============================] - 0s 61us/step - loss: 0.0470 - acc: 0.9911 - val_loss: 1.1480 - val_acc: 0.6145\n",
            "Epoch 41/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.0445 - acc: 0.9923 - val_loss: 1.1622 - val_acc: 0.6183\n",
            "Epoch 42/50\n",
            "784/784 [==============================] - 0s 48us/step - loss: 0.0444 - acc: 0.9898 - val_loss: 1.1676 - val_acc: 0.6183\n",
            "Epoch 43/50\n",
            "784/784 [==============================] - 0s 48us/step - loss: 0.0430 - acc: 0.9923 - val_loss: 1.1727 - val_acc: 0.6260\n",
            "Epoch 44/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.0411 - acc: 0.9898 - val_loss: 1.1755 - val_acc: 0.6183\n",
            "Epoch 45/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.0390 - acc: 0.9911 - val_loss: 1.1861 - val_acc: 0.6145\n",
            "Epoch 46/50\n",
            "784/784 [==============================] - 0s 50us/step - loss: 0.0373 - acc: 0.9911 - val_loss: 1.1997 - val_acc: 0.6031\n",
            "Epoch 47/50\n",
            "784/784 [==============================] - 0s 49us/step - loss: 0.0368 - acc: 0.9911 - val_loss: 1.2056 - val_acc: 0.6069\n",
            "Epoch 48/50\n",
            "784/784 [==============================] - 0s 48us/step - loss: 0.0342 - acc: 0.9898 - val_loss: 1.2097 - val_acc: 0.6183\n",
            "Epoch 49/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.0335 - acc: 0.9923 - val_loss: 1.2246 - val_acc: 0.6183\n",
            "Epoch 50/50\n",
            "784/784 [==============================] - 0s 51us/step - loss: 0.0343 - acc: 0.9898 - val_loss: 1.2199 - val_acc: 0.6107\n",
            "Test loss: 1.2198722776565842\n",
            "Test accuracy: 0.6106870233557606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vCnB39kyLzZg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
        "\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec.vocab:\n",
        "        embedding_matrix[i] = word2vec.word_vec(word)\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
        "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc3zgKIyg-D_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "09db3021-0ff0-4db8-80d5-7edac1565a56"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(150,activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(8,activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 30, 300)           9858600   \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 30, 300)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 14, 300)           270300    \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 6, 150)            135150    \n",
            "_________________________________________________________________\n",
            "conv1d_20 (Conv1D)           (None, 2, 75)             33825     \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 150)               22650     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 8)                 1208      \n",
            "=================================================================\n",
            "Total params: 10,321,733\n",
            "Trainable params: 463,133\n",
            "Non-trainable params: 9,858,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TF1PJtWhhGC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "634c18a7-b7a3-4090-a208-586117e0f5dd"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(embedding_layer)\n",
        "model_1.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n",
        "model_1.add(GlobalMaxPooling1D())\n",
        "model_1.add(Dense(250))\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Dense(8))\n",
        "model_1.add(Activation('sigmoid'))\n",
        "model_1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
        "model_1.summary()"
      ],
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 30, 300)           9858600   \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 28, 250)           225250    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_7 (Glob (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 250)               62750     \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 8)                 2008      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 10,148,608\n",
            "Trainable params: 290,008\n",
            "Non-trainable params: 9,858,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fF_RzJcohKzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18071
        },
        "outputId": "fd1f7e71-18b8-4803-c277-d53837825527"
      },
      "cell_type": "code",
      "source": [
        "history_model = model.fit(X_train, y_train, validation_split=0.4, epochs=500, batch_size=128)\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 470 samples, validate on 314 samples\n",
            "Epoch 1/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1521 - acc: 0.9340 - val_loss: 2.4713 - val_acc: 0.5287\n",
            "Epoch 2/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1519 - acc: 0.9319 - val_loss: 2.4909 - val_acc: 0.5318\n",
            "Epoch 3/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1473 - acc: 0.9362 - val_loss: 2.5429 - val_acc: 0.5159\n",
            "Epoch 4/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1476 - acc: 0.9319 - val_loss: 2.5769 - val_acc: 0.5223\n",
            "Epoch 5/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1467 - acc: 0.9362 - val_loss: 2.5019 - val_acc: 0.5350\n",
            "Epoch 6/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1556 - acc: 0.9255 - val_loss: 2.5331 - val_acc: 0.5382\n",
            "Epoch 7/500\n",
            "470/470 [==============================] - 0s 179us/step - loss: 0.1391 - acc: 0.9426 - val_loss: 2.4548 - val_acc: 0.5032\n",
            "Epoch 8/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1441 - acc: 0.9383 - val_loss: 2.4519 - val_acc: 0.5255\n",
            "Epoch 9/500\n",
            "470/470 [==============================] - 0s 177us/step - loss: 0.1417 - acc: 0.9362 - val_loss: 2.5248 - val_acc: 0.5350\n",
            "Epoch 10/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1474 - acc: 0.9340 - val_loss: 2.6246 - val_acc: 0.5287\n",
            "Epoch 11/500\n",
            "470/470 [==============================] - 0s 198us/step - loss: 0.1500 - acc: 0.9319 - val_loss: 2.5575 - val_acc: 0.5382\n",
            "Epoch 12/500\n",
            "470/470 [==============================] - 0s 185us/step - loss: 0.1464 - acc: 0.9383 - val_loss: 2.4957 - val_acc: 0.5255\n",
            "Epoch 13/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1453 - acc: 0.9426 - val_loss: 2.5431 - val_acc: 0.5350\n",
            "Epoch 14/500\n",
            "470/470 [==============================] - 0s 183us/step - loss: 0.1469 - acc: 0.9340 - val_loss: 2.5291 - val_acc: 0.5318\n",
            "Epoch 15/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1347 - acc: 0.9383 - val_loss: 2.5254 - val_acc: 0.5287\n",
            "Epoch 16/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1449 - acc: 0.9362 - val_loss: 2.7274 - val_acc: 0.5223\n",
            "Epoch 17/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1455 - acc: 0.9340 - val_loss: 2.5996 - val_acc: 0.5350\n",
            "Epoch 18/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1473 - acc: 0.9404 - val_loss: 2.6908 - val_acc: 0.5318\n",
            "Epoch 19/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1445 - acc: 0.9426 - val_loss: 2.5179 - val_acc: 0.5446\n",
            "Epoch 20/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1485 - acc: 0.9319 - val_loss: 2.6586 - val_acc: 0.5287\n",
            "Epoch 21/500\n",
            "470/470 [==============================] - 0s 178us/step - loss: 0.1566 - acc: 0.9340 - val_loss: 2.6064 - val_acc: 0.5478\n",
            "Epoch 22/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1488 - acc: 0.9362 - val_loss: 2.5967 - val_acc: 0.5318\n",
            "Epoch 23/500\n",
            "470/470 [==============================] - 0s 194us/step - loss: 0.1479 - acc: 0.9319 - val_loss: 2.7124 - val_acc: 0.5414\n",
            "Epoch 24/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1413 - acc: 0.9447 - val_loss: 2.7047 - val_acc: 0.5414\n",
            "Epoch 25/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1563 - acc: 0.9319 - val_loss: 2.6457 - val_acc: 0.5350\n",
            "Epoch 26/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1565 - acc: 0.9277 - val_loss: 2.5986 - val_acc: 0.5414\n",
            "Epoch 27/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1437 - acc: 0.9340 - val_loss: 2.9103 - val_acc: 0.4968\n",
            "Epoch 28/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1453 - acc: 0.9298 - val_loss: 2.6458 - val_acc: 0.5223\n",
            "Epoch 29/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1451 - acc: 0.9383 - val_loss: 2.6164 - val_acc: 0.5191\n",
            "Epoch 30/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1476 - acc: 0.9319 - val_loss: 2.9786 - val_acc: 0.5064\n",
            "Epoch 31/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1391 - acc: 0.9426 - val_loss: 2.8500 - val_acc: 0.5287\n",
            "Epoch 32/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1486 - acc: 0.9277 - val_loss: 2.7209 - val_acc: 0.5287\n",
            "Epoch 33/500\n",
            "470/470 [==============================] - 0s 179us/step - loss: 0.1489 - acc: 0.9340 - val_loss: 2.7497 - val_acc: 0.5287\n",
            "Epoch 34/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1444 - acc: 0.9298 - val_loss: 2.7093 - val_acc: 0.5287\n",
            "Epoch 35/500\n",
            "470/470 [==============================] - 0s 182us/step - loss: 0.1413 - acc: 0.9404 - val_loss: 2.7018 - val_acc: 0.5318\n",
            "Epoch 36/500\n",
            "470/470 [==============================] - 0s 181us/step - loss: 0.1599 - acc: 0.9298 - val_loss: 2.6767 - val_acc: 0.5255\n",
            "Epoch 37/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1535 - acc: 0.9340 - val_loss: 2.5502 - val_acc: 0.5318\n",
            "Epoch 38/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1458 - acc: 0.9362 - val_loss: 2.6067 - val_acc: 0.5223\n",
            "Epoch 39/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1391 - acc: 0.9447 - val_loss: 2.5633 - val_acc: 0.5223\n",
            "Epoch 40/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1505 - acc: 0.9319 - val_loss: 2.6824 - val_acc: 0.5446\n",
            "Epoch 41/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1469 - acc: 0.9404 - val_loss: 2.7026 - val_acc: 0.5382\n",
            "Epoch 42/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1502 - acc: 0.9319 - val_loss: 2.7092 - val_acc: 0.5382\n",
            "Epoch 43/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1468 - acc: 0.9404 - val_loss: 2.8530 - val_acc: 0.5159\n",
            "Epoch 44/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1485 - acc: 0.9277 - val_loss: 2.6555 - val_acc: 0.5350\n",
            "Epoch 45/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1390 - acc: 0.9426 - val_loss: 2.6075 - val_acc: 0.5350\n",
            "Epoch 46/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1528 - acc: 0.9319 - val_loss: 2.5964 - val_acc: 0.5318\n",
            "Epoch 47/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1484 - acc: 0.9340 - val_loss: 2.6208 - val_acc: 0.5191\n",
            "Epoch 48/500\n",
            "470/470 [==============================] - 0s 196us/step - loss: 0.1604 - acc: 0.9277 - val_loss: 2.7313 - val_acc: 0.5159\n",
            "Epoch 49/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1431 - acc: 0.9340 - val_loss: 2.6582 - val_acc: 0.5318\n",
            "Epoch 50/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1435 - acc: 0.9362 - val_loss: 2.7372 - val_acc: 0.5255\n",
            "Epoch 51/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1490 - acc: 0.9340 - val_loss: 2.7102 - val_acc: 0.5223\n",
            "Epoch 52/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1481 - acc: 0.9277 - val_loss: 2.6794 - val_acc: 0.5223\n",
            "Epoch 53/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1401 - acc: 0.9426 - val_loss: 2.6906 - val_acc: 0.5350\n",
            "Epoch 54/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1418 - acc: 0.9404 - val_loss: 2.8117 - val_acc: 0.5191\n",
            "Epoch 55/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1434 - acc: 0.9298 - val_loss: 2.6672 - val_acc: 0.5255\n",
            "Epoch 56/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1463 - acc: 0.9404 - val_loss: 2.7744 - val_acc: 0.5191\n",
            "Epoch 57/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1443 - acc: 0.9383 - val_loss: 2.8265 - val_acc: 0.5191\n",
            "Epoch 58/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1453 - acc: 0.9383 - val_loss: 2.7449 - val_acc: 0.5318\n",
            "Epoch 59/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1467 - acc: 0.9426 - val_loss: 2.7928 - val_acc: 0.5191\n",
            "Epoch 60/500\n",
            "470/470 [==============================] - 0s 183us/step - loss: 0.1465 - acc: 0.9362 - val_loss: 2.6571 - val_acc: 0.5414\n",
            "Epoch 61/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1444 - acc: 0.9383 - val_loss: 2.6453 - val_acc: 0.5255\n",
            "Epoch 62/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1426 - acc: 0.9383 - val_loss: 2.7083 - val_acc: 0.5350\n",
            "Epoch 63/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1487 - acc: 0.9319 - val_loss: 2.7205 - val_acc: 0.5064\n",
            "Epoch 64/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1447 - acc: 0.9383 - val_loss: 2.7383 - val_acc: 0.5064\n",
            "Epoch 65/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1440 - acc: 0.9319 - val_loss: 2.7721 - val_acc: 0.5096\n",
            "Epoch 66/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1464 - acc: 0.9404 - val_loss: 2.8132 - val_acc: 0.5287\n",
            "Epoch 67/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1524 - acc: 0.9298 - val_loss: 2.7889 - val_acc: 0.5287\n",
            "Epoch 68/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1441 - acc: 0.9383 - val_loss: 2.7118 - val_acc: 0.5255\n",
            "Epoch 69/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1399 - acc: 0.9340 - val_loss: 2.7823 - val_acc: 0.5287\n",
            "Epoch 70/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1470 - acc: 0.9340 - val_loss: 2.6727 - val_acc: 0.5350\n",
            "Epoch 71/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1467 - acc: 0.9319 - val_loss: 2.7266 - val_acc: 0.5478\n",
            "Epoch 72/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1438 - acc: 0.9404 - val_loss: 2.6625 - val_acc: 0.5446\n",
            "Epoch 73/500\n",
            "470/470 [==============================] - 0s 191us/step - loss: 0.1522 - acc: 0.9255 - val_loss: 2.6810 - val_acc: 0.5350\n",
            "Epoch 74/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1590 - acc: 0.9319 - val_loss: 2.6546 - val_acc: 0.5350\n",
            "Epoch 75/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1537 - acc: 0.9362 - val_loss: 2.7913 - val_acc: 0.5223\n",
            "Epoch 76/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1495 - acc: 0.9362 - val_loss: 2.7307 - val_acc: 0.5350\n",
            "Epoch 77/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1458 - acc: 0.9340 - val_loss: 2.8313 - val_acc: 0.5223\n",
            "Epoch 78/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1429 - acc: 0.9362 - val_loss: 2.7559 - val_acc: 0.5318\n",
            "Epoch 79/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1440 - acc: 0.9362 - val_loss: 2.7622 - val_acc: 0.5350\n",
            "Epoch 80/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1450 - acc: 0.9426 - val_loss: 2.6958 - val_acc: 0.5318\n",
            "Epoch 81/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1501 - acc: 0.9340 - val_loss: 2.8267 - val_acc: 0.5255\n",
            "Epoch 82/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1441 - acc: 0.9362 - val_loss: 2.7456 - val_acc: 0.5287\n",
            "Epoch 83/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1527 - acc: 0.9383 - val_loss: 2.6300 - val_acc: 0.5287\n",
            "Epoch 84/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1480 - acc: 0.9383 - val_loss: 2.7626 - val_acc: 0.5127\n",
            "Epoch 85/500\n",
            "470/470 [==============================] - 0s 193us/step - loss: 0.1457 - acc: 0.9426 - val_loss: 2.7974 - val_acc: 0.5191\n",
            "Epoch 86/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1514 - acc: 0.9277 - val_loss: 2.6980 - val_acc: 0.5350\n",
            "Epoch 87/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1337 - acc: 0.9383 - val_loss: 2.7526 - val_acc: 0.5287\n",
            "Epoch 88/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1489 - acc: 0.9383 - val_loss: 2.8448 - val_acc: 0.5287\n",
            "Epoch 89/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1611 - acc: 0.9383 - val_loss: 2.7376 - val_acc: 0.5223\n",
            "Epoch 90/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1462 - acc: 0.9340 - val_loss: 2.6543 - val_acc: 0.5318\n",
            "Epoch 91/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1420 - acc: 0.9362 - val_loss: 2.6326 - val_acc: 0.5350\n",
            "Epoch 92/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1453 - acc: 0.9383 - val_loss: 2.7021 - val_acc: 0.5350\n",
            "Epoch 93/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1379 - acc: 0.9447 - val_loss: 2.6573 - val_acc: 0.5255\n",
            "Epoch 94/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1562 - acc: 0.9362 - val_loss: 2.7762 - val_acc: 0.5223\n",
            "Epoch 95/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1417 - acc: 0.9340 - val_loss: 2.7333 - val_acc: 0.5255\n",
            "Epoch 96/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1510 - acc: 0.9340 - val_loss: 2.7581 - val_acc: 0.5191\n",
            "Epoch 97/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1412 - acc: 0.9383 - val_loss: 2.7550 - val_acc: 0.5318\n",
            "Epoch 98/500\n",
            "470/470 [==============================] - 0s 188us/step - loss: 0.1431 - acc: 0.9426 - val_loss: 2.8327 - val_acc: 0.5223\n",
            "Epoch 99/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1502 - acc: 0.9383 - val_loss: 2.8722 - val_acc: 0.5127\n",
            "Epoch 100/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1472 - acc: 0.9298 - val_loss: 3.0441 - val_acc: 0.5096\n",
            "Epoch 101/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1475 - acc: 0.9319 - val_loss: 2.9093 - val_acc: 0.5064\n",
            "Epoch 102/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1421 - acc: 0.9426 - val_loss: 2.9837 - val_acc: 0.5127\n",
            "Epoch 103/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1466 - acc: 0.9340 - val_loss: 2.7258 - val_acc: 0.5191\n",
            "Epoch 104/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1487 - acc: 0.9362 - val_loss: 2.6771 - val_acc: 0.5255\n",
            "Epoch 105/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1449 - acc: 0.9362 - val_loss: 2.7579 - val_acc: 0.5287\n",
            "Epoch 106/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1450 - acc: 0.9447 - val_loss: 2.8876 - val_acc: 0.5159\n",
            "Epoch 107/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1483 - acc: 0.9340 - val_loss: 2.7129 - val_acc: 0.5223\n",
            "Epoch 108/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1444 - acc: 0.9383 - val_loss: 2.7492 - val_acc: 0.5318\n",
            "Epoch 109/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1489 - acc: 0.9362 - val_loss: 2.7467 - val_acc: 0.5382\n",
            "Epoch 110/500\n",
            "470/470 [==============================] - 0s 185us/step - loss: 0.1414 - acc: 0.9383 - val_loss: 2.7475 - val_acc: 0.5350\n",
            "Epoch 111/500\n",
            "470/470 [==============================] - 0s 183us/step - loss: 0.1322 - acc: 0.9447 - val_loss: 2.8160 - val_acc: 0.5191\n",
            "Epoch 112/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1403 - acc: 0.9426 - val_loss: 2.8136 - val_acc: 0.5414\n",
            "Epoch 113/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1500 - acc: 0.9362 - val_loss: 2.7837 - val_acc: 0.5287\n",
            "Epoch 114/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1408 - acc: 0.9404 - val_loss: 2.7887 - val_acc: 0.5287\n",
            "Epoch 115/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1449 - acc: 0.9362 - val_loss: 2.9558 - val_acc: 0.5223\n",
            "Epoch 116/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1491 - acc: 0.9362 - val_loss: 2.7098 - val_acc: 0.5318\n",
            "Epoch 117/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1476 - acc: 0.9362 - val_loss: 2.7002 - val_acc: 0.5350\n",
            "Epoch 118/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1479 - acc: 0.9255 - val_loss: 2.7813 - val_acc: 0.5414\n",
            "Epoch 119/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1450 - acc: 0.9340 - val_loss: 2.7730 - val_acc: 0.5318\n",
            "Epoch 120/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1417 - acc: 0.9426 - val_loss: 2.8534 - val_acc: 0.5159\n",
            "Epoch 121/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1474 - acc: 0.9340 - val_loss: 2.9143 - val_acc: 0.5287\n",
            "Epoch 122/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1484 - acc: 0.9362 - val_loss: 2.9574 - val_acc: 0.5159\n",
            "Epoch 123/500\n",
            "470/470 [==============================] - 0s 189us/step - loss: 0.1441 - acc: 0.9362 - val_loss: 3.1096 - val_acc: 0.5032\n",
            "Epoch 124/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1506 - acc: 0.9298 - val_loss: 3.0181 - val_acc: 0.5223\n",
            "Epoch 125/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1405 - acc: 0.9404 - val_loss: 2.9732 - val_acc: 0.5223\n",
            "Epoch 126/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1396 - acc: 0.9426 - val_loss: 2.8793 - val_acc: 0.5318\n",
            "Epoch 127/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1443 - acc: 0.9383 - val_loss: 3.0005 - val_acc: 0.5318\n",
            "Epoch 128/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1420 - acc: 0.9340 - val_loss: 3.0301 - val_acc: 0.5287\n",
            "Epoch 129/500\n",
            "470/470 [==============================] - 0s 182us/step - loss: 0.1478 - acc: 0.9383 - val_loss: 3.1331 - val_acc: 0.5255\n",
            "Epoch 130/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1459 - acc: 0.9426 - val_loss: 2.9734 - val_acc: 0.5318\n",
            "Epoch 131/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1356 - acc: 0.9404 - val_loss: 3.0165 - val_acc: 0.5287\n",
            "Epoch 132/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1447 - acc: 0.9340 - val_loss: 3.0555 - val_acc: 0.5255\n",
            "Epoch 133/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1507 - acc: 0.9298 - val_loss: 3.0511 - val_acc: 0.5223\n",
            "Epoch 134/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1475 - acc: 0.9383 - val_loss: 3.0234 - val_acc: 0.5287\n",
            "Epoch 135/500\n",
            "470/470 [==============================] - 0s 200us/step - loss: 0.1440 - acc: 0.9404 - val_loss: 2.9896 - val_acc: 0.5255\n",
            "Epoch 136/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1455 - acc: 0.9340 - val_loss: 2.8969 - val_acc: 0.5414\n",
            "Epoch 137/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1464 - acc: 0.9426 - val_loss: 2.9552 - val_acc: 0.5159\n",
            "Epoch 138/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1492 - acc: 0.9319 - val_loss: 2.9248 - val_acc: 0.5255\n",
            "Epoch 139/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1418 - acc: 0.9340 - val_loss: 3.0204 - val_acc: 0.5127\n",
            "Epoch 140/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1391 - acc: 0.9404 - val_loss: 2.8760 - val_acc: 0.5159\n",
            "Epoch 141/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1409 - acc: 0.9447 - val_loss: 2.8924 - val_acc: 0.5255\n",
            "Epoch 142/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1351 - acc: 0.9426 - val_loss: 2.9239 - val_acc: 0.5191\n",
            "Epoch 143/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1412 - acc: 0.9404 - val_loss: 2.9853 - val_acc: 0.5127\n",
            "Epoch 144/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1481 - acc: 0.9319 - val_loss: 2.9324 - val_acc: 0.5191\n",
            "Epoch 145/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1414 - acc: 0.9426 - val_loss: 2.9211 - val_acc: 0.5096\n",
            "Epoch 146/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1461 - acc: 0.9362 - val_loss: 3.0102 - val_acc: 0.5159\n",
            "Epoch 147/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1506 - acc: 0.9319 - val_loss: 3.0959 - val_acc: 0.5096\n",
            "Epoch 148/500\n",
            "470/470 [==============================] - 0s 184us/step - loss: 0.1394 - acc: 0.9426 - val_loss: 3.0073 - val_acc: 0.5064\n",
            "Epoch 149/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1344 - acc: 0.9362 - val_loss: 2.9605 - val_acc: 0.5159\n",
            "Epoch 150/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1464 - acc: 0.9319 - val_loss: 2.9836 - val_acc: 0.5223\n",
            "Epoch 151/500\n",
            "470/470 [==============================] - 0s 180us/step - loss: 0.1408 - acc: 0.9362 - val_loss: 2.9977 - val_acc: 0.5191\n",
            "Epoch 152/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1477 - acc: 0.9383 - val_loss: 3.0725 - val_acc: 0.5096\n",
            "Epoch 153/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1454 - acc: 0.9340 - val_loss: 2.9757 - val_acc: 0.5350\n",
            "Epoch 154/500\n",
            "470/470 [==============================] - 0s 184us/step - loss: 0.1407 - acc: 0.9426 - val_loss: 2.9685 - val_acc: 0.5287\n",
            "Epoch 155/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1427 - acc: 0.9426 - val_loss: 2.9433 - val_acc: 0.5350\n",
            "Epoch 156/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1494 - acc: 0.9340 - val_loss: 3.0401 - val_acc: 0.5255\n",
            "Epoch 157/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1508 - acc: 0.9319 - val_loss: 2.9503 - val_acc: 0.5255\n",
            "Epoch 158/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1502 - acc: 0.9319 - val_loss: 3.1102 - val_acc: 0.5096\n",
            "Epoch 159/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1432 - acc: 0.9362 - val_loss: 3.0355 - val_acc: 0.5064\n",
            "Epoch 160/500\n",
            "470/470 [==============================] - 0s 185us/step - loss: 0.1492 - acc: 0.9340 - val_loss: 3.1141 - val_acc: 0.5191\n",
            "Epoch 161/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1428 - acc: 0.9362 - val_loss: 3.2848 - val_acc: 0.5096\n",
            "Epoch 162/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1357 - acc: 0.9426 - val_loss: 3.1896 - val_acc: 0.5127\n",
            "Epoch 163/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1402 - acc: 0.9426 - val_loss: 3.2016 - val_acc: 0.5096\n",
            "Epoch 164/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1452 - acc: 0.9362 - val_loss: 3.1484 - val_acc: 0.5191\n",
            "Epoch 165/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1438 - acc: 0.9319 - val_loss: 3.1691 - val_acc: 0.5255\n",
            "Epoch 166/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1451 - acc: 0.9319 - val_loss: 3.1622 - val_acc: 0.5191\n",
            "Epoch 167/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1377 - acc: 0.9404 - val_loss: 3.1500 - val_acc: 0.5191\n",
            "Epoch 168/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1451 - acc: 0.9362 - val_loss: 3.1072 - val_acc: 0.5318\n",
            "Epoch 169/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1364 - acc: 0.9404 - val_loss: 3.1724 - val_acc: 0.5096\n",
            "Epoch 170/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1395 - acc: 0.9362 - val_loss: 3.1678 - val_acc: 0.5159\n",
            "Epoch 171/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1408 - acc: 0.9404 - val_loss: 3.1569 - val_acc: 0.5064\n",
            "Epoch 172/500\n",
            "470/470 [==============================] - 0s 160us/step - loss: 0.1450 - acc: 0.9340 - val_loss: 3.1150 - val_acc: 0.5032\n",
            "Epoch 173/500\n",
            "470/470 [==============================] - 0s 192us/step - loss: 0.1462 - acc: 0.9404 - val_loss: 3.0578 - val_acc: 0.5223\n",
            "Epoch 174/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1441 - acc: 0.9404 - val_loss: 3.0639 - val_acc: 0.5159\n",
            "Epoch 175/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1514 - acc: 0.9319 - val_loss: 3.2136 - val_acc: 0.5000\n",
            "Epoch 176/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1429 - acc: 0.9426 - val_loss: 3.0347 - val_acc: 0.5032\n",
            "Epoch 177/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1424 - acc: 0.9340 - val_loss: 3.0751 - val_acc: 0.5032\n",
            "Epoch 178/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1427 - acc: 0.9298 - val_loss: 3.0608 - val_acc: 0.5096\n",
            "Epoch 179/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1441 - acc: 0.9383 - val_loss: 2.9370 - val_acc: 0.5255\n",
            "Epoch 180/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1434 - acc: 0.9426 - val_loss: 3.0187 - val_acc: 0.5159\n",
            "Epoch 181/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1492 - acc: 0.9319 - val_loss: 2.9796 - val_acc: 0.5255\n",
            "Epoch 182/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1484 - acc: 0.9340 - val_loss: 3.0981 - val_acc: 0.5318\n",
            "Epoch 183/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1410 - acc: 0.9447 - val_loss: 3.0646 - val_acc: 0.5414\n",
            "Epoch 184/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1417 - acc: 0.9383 - val_loss: 3.0588 - val_acc: 0.5287\n",
            "Epoch 185/500\n",
            "470/470 [==============================] - 0s 186us/step - loss: 0.1400 - acc: 0.9426 - val_loss: 3.2039 - val_acc: 0.5255\n",
            "Epoch 186/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1439 - acc: 0.9383 - val_loss: 3.2083 - val_acc: 0.5159\n",
            "Epoch 187/500\n",
            "470/470 [==============================] - 0s 177us/step - loss: 0.1455 - acc: 0.9362 - val_loss: 3.1235 - val_acc: 0.5191\n",
            "Epoch 188/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1450 - acc: 0.9362 - val_loss: 3.1256 - val_acc: 0.5255\n",
            "Epoch 189/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1403 - acc: 0.9362 - val_loss: 3.1073 - val_acc: 0.5223\n",
            "Epoch 190/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1511 - acc: 0.9277 - val_loss: 3.0621 - val_acc: 0.5223\n",
            "Epoch 191/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1460 - acc: 0.9383 - val_loss: 3.1292 - val_acc: 0.5191\n",
            "Epoch 192/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1405 - acc: 0.9383 - val_loss: 3.1539 - val_acc: 0.5191\n",
            "Epoch 193/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1453 - acc: 0.9340 - val_loss: 3.1255 - val_acc: 0.5064\n",
            "Epoch 194/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1459 - acc: 0.9404 - val_loss: 3.0992 - val_acc: 0.5096\n",
            "Epoch 195/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1443 - acc: 0.9426 - val_loss: 3.0942 - val_acc: 0.5127\n",
            "Epoch 196/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1410 - acc: 0.9383 - val_loss: 3.0894 - val_acc: 0.5159\n",
            "Epoch 197/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1389 - acc: 0.9404 - val_loss: 3.0123 - val_acc: 0.5223\n",
            "Epoch 198/500\n",
            "470/470 [==============================] - 0s 194us/step - loss: 0.1471 - acc: 0.9319 - val_loss: 3.0713 - val_acc: 0.5159\n",
            "Epoch 199/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1522 - acc: 0.9319 - val_loss: 3.0100 - val_acc: 0.5096\n",
            "Epoch 200/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1408 - acc: 0.9362 - val_loss: 3.0527 - val_acc: 0.5159\n",
            "Epoch 201/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1480 - acc: 0.9404 - val_loss: 3.0978 - val_acc: 0.5159\n",
            "Epoch 202/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1440 - acc: 0.9404 - val_loss: 3.1538 - val_acc: 0.5127\n",
            "Epoch 203/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1395 - acc: 0.9383 - val_loss: 3.0981 - val_acc: 0.5159\n",
            "Epoch 204/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1416 - acc: 0.9362 - val_loss: 3.0457 - val_acc: 0.4968\n",
            "Epoch 205/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1352 - acc: 0.9362 - val_loss: 3.1254 - val_acc: 0.5000\n",
            "Epoch 206/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1399 - acc: 0.9447 - val_loss: 3.1428 - val_acc: 0.5032\n",
            "Epoch 207/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1435 - acc: 0.9383 - val_loss: 3.1258 - val_acc: 0.5096\n",
            "Epoch 208/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1469 - acc: 0.9362 - val_loss: 3.0957 - val_acc: 0.5255\n",
            "Epoch 209/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1550 - acc: 0.9404 - val_loss: 3.0935 - val_acc: 0.5127\n",
            "Epoch 210/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1370 - acc: 0.9383 - val_loss: 3.0786 - val_acc: 0.5191\n",
            "Epoch 211/500\n",
            "470/470 [==============================] - 0s 190us/step - loss: 0.1515 - acc: 0.9362 - val_loss: 3.0566 - val_acc: 0.5191\n",
            "Epoch 212/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1463 - acc: 0.9362 - val_loss: 3.1740 - val_acc: 0.5223\n",
            "Epoch 213/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1362 - acc: 0.9426 - val_loss: 3.0587 - val_acc: 0.5191\n",
            "Epoch 214/500\n",
            "470/470 [==============================] - 0s 177us/step - loss: 0.1448 - acc: 0.9404 - val_loss: 3.0681 - val_acc: 0.5159\n",
            "Epoch 215/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1394 - acc: 0.9383 - val_loss: 3.0282 - val_acc: 0.5287\n",
            "Epoch 216/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1417 - acc: 0.9319 - val_loss: 3.0110 - val_acc: 0.5159\n",
            "Epoch 217/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1479 - acc: 0.9362 - val_loss: 3.0819 - val_acc: 0.5159\n",
            "Epoch 218/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1447 - acc: 0.9447 - val_loss: 3.2262 - val_acc: 0.5191\n",
            "Epoch 219/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1456 - acc: 0.9404 - val_loss: 3.1235 - val_acc: 0.5096\n",
            "Epoch 220/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1433 - acc: 0.9362 - val_loss: 3.1113 - val_acc: 0.5191\n",
            "Epoch 221/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1386 - acc: 0.9362 - val_loss: 3.1346 - val_acc: 0.5223\n",
            "Epoch 222/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1437 - acc: 0.9404 - val_loss: 3.1351 - val_acc: 0.5127\n",
            "Epoch 223/500\n",
            "470/470 [==============================] - 0s 182us/step - loss: 0.1450 - acc: 0.9340 - val_loss: 3.2008 - val_acc: 0.5096\n",
            "Epoch 224/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1400 - acc: 0.9426 - val_loss: 3.0526 - val_acc: 0.5255\n",
            "Epoch 225/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1431 - acc: 0.9447 - val_loss: 3.2688 - val_acc: 0.5096\n",
            "Epoch 226/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1440 - acc: 0.9319 - val_loss: 3.2452 - val_acc: 0.5064\n",
            "Epoch 227/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1400 - acc: 0.9404 - val_loss: 3.2600 - val_acc: 0.5064\n",
            "Epoch 228/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1433 - acc: 0.9362 - val_loss: 3.2343 - val_acc: 0.5159\n",
            "Epoch 229/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1420 - acc: 0.9383 - val_loss: 3.2099 - val_acc: 0.5223\n",
            "Epoch 230/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1486 - acc: 0.9340 - val_loss: 3.1984 - val_acc: 0.5032\n",
            "Epoch 231/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1395 - acc: 0.9362 - val_loss: 3.2112 - val_acc: 0.5127\n",
            "Epoch 232/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1419 - acc: 0.9426 - val_loss: 3.2170 - val_acc: 0.5159\n",
            "Epoch 233/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1400 - acc: 0.9362 - val_loss: 3.2398 - val_acc: 0.5159\n",
            "Epoch 234/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1421 - acc: 0.9383 - val_loss: 3.1521 - val_acc: 0.5191\n",
            "Epoch 235/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1457 - acc: 0.9404 - val_loss: 3.1361 - val_acc: 0.5159\n",
            "Epoch 236/500\n",
            "470/470 [==============================] - 0s 186us/step - loss: 0.1442 - acc: 0.9340 - val_loss: 3.2011 - val_acc: 0.5255\n",
            "Epoch 237/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1408 - acc: 0.9404 - val_loss: 3.1742 - val_acc: 0.5159\n",
            "Epoch 238/500\n",
            "470/470 [==============================] - 0s 178us/step - loss: 0.1437 - acc: 0.9404 - val_loss: 3.2328 - val_acc: 0.5191\n",
            "Epoch 239/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1470 - acc: 0.9383 - val_loss: 3.2231 - val_acc: 0.5127\n",
            "Epoch 240/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1384 - acc: 0.9404 - val_loss: 3.2467 - val_acc: 0.5159\n",
            "Epoch 241/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1398 - acc: 0.9404 - val_loss: 3.2411 - val_acc: 0.5159\n",
            "Epoch 242/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1384 - acc: 0.9383 - val_loss: 3.3386 - val_acc: 0.4968\n",
            "Epoch 243/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1436 - acc: 0.9404 - val_loss: 3.2350 - val_acc: 0.5191\n",
            "Epoch 244/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1423 - acc: 0.9404 - val_loss: 3.2452 - val_acc: 0.5159\n",
            "Epoch 245/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1420 - acc: 0.9383 - val_loss: 3.2635 - val_acc: 0.5191\n",
            "Epoch 246/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1426 - acc: 0.9404 - val_loss: 3.1751 - val_acc: 0.5223\n",
            "Epoch 247/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1410 - acc: 0.9404 - val_loss: 3.1934 - val_acc: 0.5223\n",
            "Epoch 248/500\n",
            "470/470 [==============================] - 0s 189us/step - loss: 0.1390 - acc: 0.9383 - val_loss: 3.1868 - val_acc: 0.5191\n",
            "Epoch 249/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1409 - acc: 0.9426 - val_loss: 3.1465 - val_acc: 0.5223\n",
            "Epoch 250/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1430 - acc: 0.9383 - val_loss: 3.2204 - val_acc: 0.5287\n",
            "Epoch 251/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1444 - acc: 0.9383 - val_loss: 3.2691 - val_acc: 0.5223\n",
            "Epoch 252/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1396 - acc: 0.9383 - val_loss: 3.2996 - val_acc: 0.5159\n",
            "Epoch 253/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1389 - acc: 0.9404 - val_loss: 3.3252 - val_acc: 0.5191\n",
            "Epoch 254/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1385 - acc: 0.9383 - val_loss: 3.3161 - val_acc: 0.5287\n",
            "Epoch 255/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1367 - acc: 0.9383 - val_loss: 3.2981 - val_acc: 0.5096\n",
            "Epoch 256/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1453 - acc: 0.9383 - val_loss: 3.2338 - val_acc: 0.5223\n",
            "Epoch 257/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1378 - acc: 0.9362 - val_loss: 3.2945 - val_acc: 0.5191\n",
            "Epoch 258/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1422 - acc: 0.9404 - val_loss: 3.2349 - val_acc: 0.5223\n",
            "Epoch 259/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1408 - acc: 0.9404 - val_loss: 3.3567 - val_acc: 0.5000\n",
            "Epoch 260/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1405 - acc: 0.9383 - val_loss: 3.3136 - val_acc: 0.5223\n",
            "Epoch 261/500\n",
            "470/470 [==============================] - 0s 187us/step - loss: 0.1421 - acc: 0.9383 - val_loss: 3.3282 - val_acc: 0.5127\n",
            "Epoch 262/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1407 - acc: 0.9383 - val_loss: 3.3619 - val_acc: 0.5191\n",
            "Epoch 263/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1413 - acc: 0.9404 - val_loss: 3.3044 - val_acc: 0.5159\n",
            "Epoch 264/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1403 - acc: 0.9404 - val_loss: 3.3521 - val_acc: 0.5191\n",
            "Epoch 265/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1448 - acc: 0.9404 - val_loss: 3.2788 - val_acc: 0.5223\n",
            "Epoch 266/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1421 - acc: 0.9383 - val_loss: 3.1390 - val_acc: 0.5096\n",
            "Epoch 267/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1422 - acc: 0.9426 - val_loss: 3.1316 - val_acc: 0.5255\n",
            "Epoch 268/500\n",
            "470/470 [==============================] - 0s 183us/step - loss: 0.1391 - acc: 0.9404 - val_loss: 3.2174 - val_acc: 0.5255\n",
            "Epoch 269/500\n",
            "470/470 [==============================] - 0s 195us/step - loss: 0.1419 - acc: 0.9426 - val_loss: 3.1917 - val_acc: 0.5191\n",
            "Epoch 270/500\n",
            "470/470 [==============================] - 0s 177us/step - loss: 0.1392 - acc: 0.9404 - val_loss: 2.9922 - val_acc: 0.5159\n",
            "Epoch 271/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1420 - acc: 0.9383 - val_loss: 3.4234 - val_acc: 0.5223\n",
            "Epoch 272/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1388 - acc: 0.9404 - val_loss: 3.4409 - val_acc: 0.5159\n",
            "Epoch 273/500\n",
            "470/470 [==============================] - 0s 192us/step - loss: 0.1383 - acc: 0.9383 - val_loss: 3.4526 - val_acc: 0.5127\n",
            "Epoch 274/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1391 - acc: 0.9383 - val_loss: 3.4090 - val_acc: 0.4904\n",
            "Epoch 275/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1410 - acc: 0.9383 - val_loss: 3.4151 - val_acc: 0.5191\n",
            "Epoch 276/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1386 - acc: 0.9383 - val_loss: 3.3597 - val_acc: 0.5096\n",
            "Epoch 277/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1415 - acc: 0.9383 - val_loss: 3.3566 - val_acc: 0.5223\n",
            "Epoch 278/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1382 - acc: 0.9426 - val_loss: 3.4570 - val_acc: 0.5096\n",
            "Epoch 279/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1415 - acc: 0.9383 - val_loss: 3.4820 - val_acc: 0.5064\n",
            "Epoch 280/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1393 - acc: 0.9404 - val_loss: 3.4070 - val_acc: 0.5191\n",
            "Epoch 281/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1402 - acc: 0.9383 - val_loss: 3.4696 - val_acc: 0.4968\n",
            "Epoch 282/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1414 - acc: 0.9404 - val_loss: 3.5629 - val_acc: 0.5000\n",
            "Epoch 283/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1399 - acc: 0.9404 - val_loss: 3.5329 - val_acc: 0.5032\n",
            "Epoch 284/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1437 - acc: 0.9383 - val_loss: 3.5250 - val_acc: 0.5096\n",
            "Epoch 285/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1405 - acc: 0.9362 - val_loss: 3.4959 - val_acc: 0.5064\n",
            "Epoch 286/500\n",
            "470/470 [==============================] - 0s 192us/step - loss: 0.1357 - acc: 0.9383 - val_loss: 3.4722 - val_acc: 0.5096\n",
            "Epoch 287/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1407 - acc: 0.9383 - val_loss: 3.5184 - val_acc: 0.5191\n",
            "Epoch 288/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1442 - acc: 0.9362 - val_loss: 3.4890 - val_acc: 0.5255\n",
            "Epoch 289/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1429 - acc: 0.9362 - val_loss: 3.4716 - val_acc: 0.5287\n",
            "Epoch 290/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1419 - acc: 0.9383 - val_loss: 3.5283 - val_acc: 0.5318\n",
            "Epoch 291/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1428 - acc: 0.9383 - val_loss: 3.5449 - val_acc: 0.5096\n",
            "Epoch 292/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1383 - acc: 0.9404 - val_loss: 3.5521 - val_acc: 0.5096\n",
            "Epoch 293/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1396 - acc: 0.9383 - val_loss: 3.5372 - val_acc: 0.5159\n",
            "Epoch 294/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1382 - acc: 0.9383 - val_loss: 3.5516 - val_acc: 0.5255\n",
            "Epoch 295/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1385 - acc: 0.9426 - val_loss: 3.5473 - val_acc: 0.5191\n",
            "Epoch 296/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1400 - acc: 0.9383 - val_loss: 3.4845 - val_acc: 0.5350\n",
            "Epoch 297/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1398 - acc: 0.9404 - val_loss: 3.6015 - val_acc: 0.5255\n",
            "Epoch 298/500\n",
            "470/470 [==============================] - 0s 182us/step - loss: 0.1388 - acc: 0.9426 - val_loss: 3.6045 - val_acc: 0.5287\n",
            "Epoch 299/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1385 - acc: 0.9404 - val_loss: 3.5033 - val_acc: 0.5159\n",
            "Epoch 300/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1434 - acc: 0.9383 - val_loss: 3.5838 - val_acc: 0.5159\n",
            "Epoch 301/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1425 - acc: 0.9404 - val_loss: 3.4824 - val_acc: 0.5223\n",
            "Epoch 302/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1452 - acc: 0.9404 - val_loss: 3.6371 - val_acc: 0.5191\n",
            "Epoch 303/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1392 - acc: 0.9426 - val_loss: 3.4815 - val_acc: 0.5223\n",
            "Epoch 304/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1392 - acc: 0.9404 - val_loss: 3.5621 - val_acc: 0.5223\n",
            "Epoch 305/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1420 - acc: 0.9383 - val_loss: 3.5572 - val_acc: 0.5159\n",
            "Epoch 306/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1436 - acc: 0.9404 - val_loss: 3.5244 - val_acc: 0.5127\n",
            "Epoch 307/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1384 - acc: 0.9404 - val_loss: 3.4880 - val_acc: 0.5159\n",
            "Epoch 308/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1381 - acc: 0.9383 - val_loss: 3.6173 - val_acc: 0.5127\n",
            "Epoch 309/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1364 - acc: 0.9404 - val_loss: 3.6037 - val_acc: 0.5159\n",
            "Epoch 310/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1397 - acc: 0.9404 - val_loss: 3.4296 - val_acc: 0.5064\n",
            "Epoch 311/500\n",
            "470/470 [==============================] - 0s 217us/step - loss: 0.1401 - acc: 0.9404 - val_loss: 3.7399 - val_acc: 0.5000\n",
            "Epoch 312/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1417 - acc: 0.9383 - val_loss: 3.5728 - val_acc: 0.5096\n",
            "Epoch 313/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1440 - acc: 0.9404 - val_loss: 3.4581 - val_acc: 0.5096\n",
            "Epoch 314/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1407 - acc: 0.9404 - val_loss: 3.3945 - val_acc: 0.5191\n",
            "Epoch 315/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1390 - acc: 0.9404 - val_loss: 3.4070 - val_acc: 0.5255\n",
            "Epoch 316/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1361 - acc: 0.9404 - val_loss: 3.6136 - val_acc: 0.5191\n",
            "Epoch 317/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1387 - acc: 0.9426 - val_loss: 3.5160 - val_acc: 0.5096\n",
            "Epoch 318/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1398 - acc: 0.9426 - val_loss: 3.5915 - val_acc: 0.5287\n",
            "Epoch 319/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1417 - acc: 0.9404 - val_loss: 3.6897 - val_acc: 0.4968\n",
            "Epoch 320/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1396 - acc: 0.9404 - val_loss: 3.7442 - val_acc: 0.4904\n",
            "Epoch 321/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1361 - acc: 0.9426 - val_loss: 3.7742 - val_acc: 0.5064\n",
            "Epoch 322/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1427 - acc: 0.9426 - val_loss: 3.8148 - val_acc: 0.5000\n",
            "Epoch 323/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1426 - acc: 0.9404 - val_loss: 3.7187 - val_acc: 0.5000\n",
            "Epoch 324/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1415 - acc: 0.9404 - val_loss: 3.6388 - val_acc: 0.5064\n",
            "Epoch 325/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1405 - acc: 0.9404 - val_loss: 3.7050 - val_acc: 0.5064\n",
            "Epoch 326/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1394 - acc: 0.9404 - val_loss: 3.5310 - val_acc: 0.5032\n",
            "Epoch 327/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1405 - acc: 0.9383 - val_loss: 3.5543 - val_acc: 0.5191\n",
            "Epoch 328/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1417 - acc: 0.9383 - val_loss: 3.8600 - val_acc: 0.5127\n",
            "Epoch 329/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1394 - acc: 0.9404 - val_loss: 3.8927 - val_acc: 0.5000\n",
            "Epoch 330/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1396 - acc: 0.9362 - val_loss: 3.9535 - val_acc: 0.4936\n",
            "Epoch 331/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1390 - acc: 0.9362 - val_loss: 3.9211 - val_acc: 0.4968\n",
            "Epoch 332/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1414 - acc: 0.9426 - val_loss: 3.9844 - val_acc: 0.5000\n",
            "Epoch 333/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1428 - acc: 0.9404 - val_loss: 3.9988 - val_acc: 0.5127\n",
            "Epoch 334/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1391 - acc: 0.9404 - val_loss: 4.0655 - val_acc: 0.5159\n",
            "Epoch 335/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1388 - acc: 0.9383 - val_loss: 4.0759 - val_acc: 0.5127\n",
            "Epoch 336/500\n",
            "470/470 [==============================] - 0s 201us/step - loss: 0.1381 - acc: 0.9426 - val_loss: 4.0030 - val_acc: 0.5159\n",
            "Epoch 337/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1394 - acc: 0.9362 - val_loss: 3.8380 - val_acc: 0.5255\n",
            "Epoch 338/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1401 - acc: 0.9383 - val_loss: 3.5596 - val_acc: 0.5255\n",
            "Epoch 339/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1442 - acc: 0.9383 - val_loss: 3.7996 - val_acc: 0.5159\n",
            "Epoch 340/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1386 - acc: 0.9404 - val_loss: 3.7679 - val_acc: 0.5159\n",
            "Epoch 341/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1412 - acc: 0.9404 - val_loss: 3.7734 - val_acc: 0.5064\n",
            "Epoch 342/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1432 - acc: 0.9383 - val_loss: 3.7907 - val_acc: 0.5064\n",
            "Epoch 343/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1392 - acc: 0.9383 - val_loss: 3.9798 - val_acc: 0.5159\n",
            "Epoch 344/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1437 - acc: 0.9426 - val_loss: 3.7570 - val_acc: 0.5127\n",
            "Epoch 345/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1399 - acc: 0.9404 - val_loss: 3.7773 - val_acc: 0.5032\n",
            "Epoch 346/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1443 - acc: 0.9404 - val_loss: 3.8349 - val_acc: 0.5096\n",
            "Epoch 347/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1364 - acc: 0.9404 - val_loss: 3.7775 - val_acc: 0.5191\n",
            "Epoch 348/500\n",
            "470/470 [==============================] - 0s 184us/step - loss: 0.1419 - acc: 0.9383 - val_loss: 3.7309 - val_acc: 0.5255\n",
            "Epoch 349/500\n",
            "470/470 [==============================] - 0s 179us/step - loss: 0.1383 - acc: 0.9404 - val_loss: 3.6972 - val_acc: 0.5064\n",
            "Epoch 350/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1402 - acc: 0.9404 - val_loss: 3.8664 - val_acc: 0.5191\n",
            "Epoch 351/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1420 - acc: 0.9383 - val_loss: 3.9091 - val_acc: 0.5064\n",
            "Epoch 352/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1403 - acc: 0.9426 - val_loss: 3.9232 - val_acc: 0.5159\n",
            "Epoch 353/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1379 - acc: 0.9426 - val_loss: 3.9572 - val_acc: 0.5127\n",
            "Epoch 354/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1416 - acc: 0.9404 - val_loss: 3.9229 - val_acc: 0.5191\n",
            "Epoch 355/500\n",
            "470/470 [==============================] - 0s 162us/step - loss: 0.1465 - acc: 0.9404 - val_loss: 3.9848 - val_acc: 0.5223\n",
            "Epoch 356/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1416 - acc: 0.9404 - val_loss: 4.0390 - val_acc: 0.5127\n",
            "Epoch 357/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1385 - acc: 0.9404 - val_loss: 4.0297 - val_acc: 0.5127\n",
            "Epoch 358/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1401 - acc: 0.9383 - val_loss: 4.3379 - val_acc: 0.5096\n",
            "Epoch 359/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1393 - acc: 0.9426 - val_loss: 4.2849 - val_acc: 0.5191\n",
            "Epoch 360/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1381 - acc: 0.9426 - val_loss: 4.3296 - val_acc: 0.5064\n",
            "Epoch 361/500\n",
            "470/470 [==============================] - 0s 188us/step - loss: 0.1374 - acc: 0.9426 - val_loss: 4.1870 - val_acc: 0.5096\n",
            "Epoch 362/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1383 - acc: 0.9426 - val_loss: 4.1256 - val_acc: 0.5096\n",
            "Epoch 363/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1425 - acc: 0.9404 - val_loss: 4.1371 - val_acc: 0.5223\n",
            "Epoch 364/500\n",
            "470/470 [==============================] - 0s 178us/step - loss: 0.1487 - acc: 0.9383 - val_loss: 4.0832 - val_acc: 0.4522\n",
            "Epoch 365/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1382 - acc: 0.9404 - val_loss: 4.0553 - val_acc: 0.4873\n",
            "Epoch 366/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1414 - acc: 0.9426 - val_loss: 4.0544 - val_acc: 0.4904\n",
            "Epoch 367/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1401 - acc: 0.9426 - val_loss: 4.1047 - val_acc: 0.4904\n",
            "Epoch 368/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1410 - acc: 0.9404 - val_loss: 4.0656 - val_acc: 0.4904\n",
            "Epoch 369/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1412 - acc: 0.9404 - val_loss: 4.1035 - val_acc: 0.4936\n",
            "Epoch 370/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1406 - acc: 0.9362 - val_loss: 4.1118 - val_acc: 0.4936\n",
            "Epoch 371/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1424 - acc: 0.9404 - val_loss: 4.1134 - val_acc: 0.5000\n",
            "Epoch 372/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1425 - acc: 0.9383 - val_loss: 4.1827 - val_acc: 0.5064\n",
            "Epoch 373/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1399 - acc: 0.9404 - val_loss: 4.1179 - val_acc: 0.5064\n",
            "Epoch 374/500\n",
            "470/470 [==============================] - 0s 194us/step - loss: 0.1387 - acc: 0.9426 - val_loss: 4.1619 - val_acc: 0.5000\n",
            "Epoch 375/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1426 - acc: 0.9383 - val_loss: 4.1449 - val_acc: 0.4968\n",
            "Epoch 376/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1442 - acc: 0.9404 - val_loss: 4.2031 - val_acc: 0.5064\n",
            "Epoch 377/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1405 - acc: 0.9426 - val_loss: 4.1865 - val_acc: 0.5064\n",
            "Epoch 378/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1375 - acc: 0.9426 - val_loss: 4.0461 - val_acc: 0.5096\n",
            "Epoch 379/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1451 - acc: 0.9383 - val_loss: 4.0398 - val_acc: 0.5032\n",
            "Epoch 380/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1442 - acc: 0.9362 - val_loss: 4.1086 - val_acc: 0.5032\n",
            "Epoch 381/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1420 - acc: 0.9404 - val_loss: 4.0780 - val_acc: 0.5000\n",
            "Epoch 382/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1424 - acc: 0.9383 - val_loss: 4.1468 - val_acc: 0.5096\n",
            "Epoch 383/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1410 - acc: 0.9404 - val_loss: 4.4806 - val_acc: 0.4841\n",
            "Epoch 384/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1409 - acc: 0.9383 - val_loss: 4.4020 - val_acc: 0.5064\n",
            "Epoch 385/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1427 - acc: 0.9404 - val_loss: 4.2990 - val_acc: 0.5000\n",
            "Epoch 386/500\n",
            "470/470 [==============================] - 0s 191us/step - loss: 0.1377 - acc: 0.9426 - val_loss: 4.2872 - val_acc: 0.5064\n",
            "Epoch 387/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1439 - acc: 0.9404 - val_loss: 4.3748 - val_acc: 0.5032\n",
            "Epoch 388/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1422 - acc: 0.9383 - val_loss: 4.2926 - val_acc: 0.5096\n",
            "Epoch 389/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1388 - acc: 0.9404 - val_loss: 4.2573 - val_acc: 0.5127\n",
            "Epoch 390/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1440 - acc: 0.9362 - val_loss: 4.2349 - val_acc: 0.5127\n",
            "Epoch 391/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1419 - acc: 0.9404 - val_loss: 4.2857 - val_acc: 0.5159\n",
            "Epoch 392/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1414 - acc: 0.9404 - val_loss: 4.2318 - val_acc: 0.5127\n",
            "Epoch 393/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1397 - acc: 0.9404 - val_loss: 4.2042 - val_acc: 0.5127\n",
            "Epoch 394/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1404 - acc: 0.9383 - val_loss: 4.1404 - val_acc: 0.5127\n",
            "Epoch 395/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1428 - acc: 0.9404 - val_loss: 4.1354 - val_acc: 0.5096\n",
            "Epoch 396/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1395 - acc: 0.9426 - val_loss: 4.1790 - val_acc: 0.5191\n",
            "Epoch 397/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1374 - acc: 0.9426 - val_loss: 4.2281 - val_acc: 0.4968\n",
            "Epoch 398/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1398 - acc: 0.9404 - val_loss: 4.1936 - val_acc: 0.5096\n",
            "Epoch 399/500\n",
            "470/470 [==============================] - 0s 188us/step - loss: 0.1369 - acc: 0.9404 - val_loss: 4.2837 - val_acc: 0.5032\n",
            "Epoch 400/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1382 - acc: 0.9426 - val_loss: 4.6138 - val_acc: 0.5191\n",
            "Epoch 401/500\n",
            "470/470 [==============================] - 0s 184us/step - loss: 0.1387 - acc: 0.9404 - val_loss: 4.4624 - val_acc: 0.5287\n",
            "Epoch 402/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1417 - acc: 0.9404 - val_loss: 4.5299 - val_acc: 0.5191\n",
            "Epoch 403/500\n",
            "470/470 [==============================] - 0s 178us/step - loss: 0.1371 - acc: 0.9404 - val_loss: 4.5140 - val_acc: 0.5287\n",
            "Epoch 404/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1422 - acc: 0.9383 - val_loss: 4.5123 - val_acc: 0.5287\n",
            "Epoch 405/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1424 - acc: 0.9404 - val_loss: 4.5898 - val_acc: 0.5127\n",
            "Epoch 406/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1421 - acc: 0.9404 - val_loss: 4.2870 - val_acc: 0.5096\n",
            "Epoch 407/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1408 - acc: 0.9426 - val_loss: 4.3035 - val_acc: 0.4936\n",
            "Epoch 408/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1418 - acc: 0.9404 - val_loss: 4.6050 - val_acc: 0.4841\n",
            "Epoch 409/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1423 - acc: 0.9383 - val_loss: 4.3868 - val_acc: 0.5127\n",
            "Epoch 410/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1394 - acc: 0.9426 - val_loss: 4.3684 - val_acc: 0.5127\n",
            "Epoch 411/500\n",
            "470/470 [==============================] - 0s 188us/step - loss: 0.1402 - acc: 0.9404 - val_loss: 4.2935 - val_acc: 0.5191\n",
            "Epoch 412/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1418 - acc: 0.9340 - val_loss: 4.2613 - val_acc: 0.5096\n",
            "Epoch 413/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1392 - acc: 0.9383 - val_loss: 3.9670 - val_acc: 0.5127\n",
            "Epoch 414/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1426 - acc: 0.9362 - val_loss: 4.0124 - val_acc: 0.5096\n",
            "Epoch 415/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1436 - acc: 0.9426 - val_loss: 4.0285 - val_acc: 0.4968\n",
            "Epoch 416/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1399 - acc: 0.9426 - val_loss: 4.0896 - val_acc: 0.5032\n",
            "Epoch 417/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1393 - acc: 0.9383 - val_loss: 4.0677 - val_acc: 0.5127\n",
            "Epoch 418/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1400 - acc: 0.9383 - val_loss: 4.2959 - val_acc: 0.5127\n",
            "Epoch 419/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1382 - acc: 0.9383 - val_loss: 4.3318 - val_acc: 0.5096\n",
            "Epoch 420/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1389 - acc: 0.9383 - val_loss: 4.2898 - val_acc: 0.5064\n",
            "Epoch 421/500\n",
            "470/470 [==============================] - 0s 204us/step - loss: 0.1418 - acc: 0.9404 - val_loss: 4.3781 - val_acc: 0.4968\n",
            "Epoch 422/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1377 - acc: 0.9426 - val_loss: 4.2174 - val_acc: 0.5000\n",
            "Epoch 423/500\n",
            "470/470 [==============================] - 0s 178us/step - loss: 0.1387 - acc: 0.9404 - val_loss: 4.1948 - val_acc: 0.5159\n",
            "Epoch 424/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1437 - acc: 0.9383 - val_loss: 4.2111 - val_acc: 0.5159\n",
            "Epoch 425/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1411 - acc: 0.9404 - val_loss: 4.2140 - val_acc: 0.5127\n",
            "Epoch 426/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1427 - acc: 0.9383 - val_loss: 4.2837 - val_acc: 0.5064\n",
            "Epoch 427/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1392 - acc: 0.9404 - val_loss: 4.3058 - val_acc: 0.5191\n",
            "Epoch 428/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1388 - acc: 0.9404 - val_loss: 4.2726 - val_acc: 0.5191\n",
            "Epoch 429/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1406 - acc: 0.9426 - val_loss: 4.2763 - val_acc: 0.5191\n",
            "Epoch 430/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1404 - acc: 0.9383 - val_loss: 4.2317 - val_acc: 0.5223\n",
            "Epoch 431/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1349 - acc: 0.9426 - val_loss: 4.1805 - val_acc: 0.5255\n",
            "Epoch 432/500\n",
            "470/470 [==============================] - 0s 163us/step - loss: 0.1452 - acc: 0.9383 - val_loss: 4.0297 - val_acc: 0.5064\n",
            "Epoch 433/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1412 - acc: 0.9383 - val_loss: 4.0951 - val_acc: 0.5191\n",
            "Epoch 434/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1376 - acc: 0.9426 - val_loss: 4.0987 - val_acc: 0.5255\n",
            "Epoch 435/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1375 - acc: 0.9426 - val_loss: 4.1030 - val_acc: 0.5127\n",
            "Epoch 436/500\n",
            "470/470 [==============================] - 0s 195us/step - loss: 0.1368 - acc: 0.9426 - val_loss: 4.0469 - val_acc: 0.5096\n",
            "Epoch 437/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1348 - acc: 0.9383 - val_loss: 4.1873 - val_acc: 0.5350\n",
            "Epoch 438/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1419 - acc: 0.9404 - val_loss: 4.1531 - val_acc: 0.5255\n",
            "Epoch 439/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1404 - acc: 0.9404 - val_loss: 4.1384 - val_acc: 0.5255\n",
            "Epoch 440/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1356 - acc: 0.9404 - val_loss: 4.0846 - val_acc: 0.5255\n",
            "Epoch 441/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1393 - acc: 0.9404 - val_loss: 4.2223 - val_acc: 0.5127\n",
            "Epoch 442/500\n",
            "470/470 [==============================] - 0s 174us/step - loss: 0.1389 - acc: 0.9362 - val_loss: 4.1193 - val_acc: 0.5223\n",
            "Epoch 443/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1408 - acc: 0.9383 - val_loss: 4.1349 - val_acc: 0.5318\n",
            "Epoch 444/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1405 - acc: 0.9404 - val_loss: 3.9093 - val_acc: 0.5191\n",
            "Epoch 445/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1404 - acc: 0.9404 - val_loss: 3.6820 - val_acc: 0.5096\n",
            "Epoch 446/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1420 - acc: 0.9383 - val_loss: 4.0021 - val_acc: 0.5032\n",
            "Epoch 447/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1395 - acc: 0.9404 - val_loss: 4.0450 - val_acc: 0.4873\n",
            "Epoch 448/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1429 - acc: 0.9362 - val_loss: 4.2048 - val_acc: 0.5255\n",
            "Epoch 449/500\n",
            "470/470 [==============================] - 0s 182us/step - loss: 0.1413 - acc: 0.9404 - val_loss: 4.1497 - val_acc: 0.5127\n",
            "Epoch 450/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1420 - acc: 0.9404 - val_loss: 4.1129 - val_acc: 0.5064\n",
            "Epoch 451/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1411 - acc: 0.9383 - val_loss: 4.2843 - val_acc: 0.5223\n",
            "Epoch 452/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1487 - acc: 0.9404 - val_loss: 4.0553 - val_acc: 0.5096\n",
            "Epoch 453/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1379 - acc: 0.9426 - val_loss: 4.0658 - val_acc: 0.5127\n",
            "Epoch 454/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1392 - acc: 0.9383 - val_loss: 4.0570 - val_acc: 0.5127\n",
            "Epoch 455/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1422 - acc: 0.9383 - val_loss: 4.1435 - val_acc: 0.5159\n",
            "Epoch 456/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1409 - acc: 0.9426 - val_loss: 4.1626 - val_acc: 0.5096\n",
            "Epoch 457/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1382 - acc: 0.9426 - val_loss: 4.1776 - val_acc: 0.5127\n",
            "Epoch 458/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1435 - acc: 0.9404 - val_loss: 4.3013 - val_acc: 0.5255\n",
            "Epoch 459/500\n",
            "470/470 [==============================] - 0s 178us/step - loss: 0.1413 - acc: 0.9426 - val_loss: 4.2493 - val_acc: 0.5287\n",
            "Epoch 460/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1369 - acc: 0.9404 - val_loss: 4.2886 - val_acc: 0.5287\n",
            "Epoch 461/500\n",
            "470/470 [==============================] - 0s 190us/step - loss: 0.1347 - acc: 0.9404 - val_loss: 4.2928 - val_acc: 0.5191\n",
            "Epoch 462/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1486 - acc: 0.9426 - val_loss: 4.2452 - val_acc: 0.5255\n",
            "Epoch 463/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1388 - acc: 0.9426 - val_loss: 4.3034 - val_acc: 0.5159\n",
            "Epoch 464/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1352 - acc: 0.9404 - val_loss: 4.3809 - val_acc: 0.5064\n",
            "Epoch 465/500\n",
            "470/470 [==============================] - 0s 177us/step - loss: 0.1366 - acc: 0.9426 - val_loss: 4.1861 - val_acc: 0.5255\n",
            "Epoch 466/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1417 - acc: 0.9404 - val_loss: 4.1848 - val_acc: 0.5064\n",
            "Epoch 467/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1399 - acc: 0.9404 - val_loss: 4.2308 - val_acc: 0.5127\n",
            "Epoch 468/500\n",
            "470/470 [==============================] - 0s 166us/step - loss: 0.1355 - acc: 0.9426 - val_loss: 4.2465 - val_acc: 0.5223\n",
            "Epoch 469/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1402 - acc: 0.9362 - val_loss: 4.3177 - val_acc: 0.5000\n",
            "Epoch 470/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1404 - acc: 0.9383 - val_loss: 4.2944 - val_acc: 0.4968\n",
            "Epoch 471/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1430 - acc: 0.9404 - val_loss: 4.3426 - val_acc: 0.4936\n",
            "Epoch 472/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1390 - acc: 0.9426 - val_loss: 4.3671 - val_acc: 0.4904\n",
            "Epoch 473/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1379 - acc: 0.9404 - val_loss: 4.5893 - val_acc: 0.4936\n",
            "Epoch 474/500\n",
            "470/470 [==============================] - 0s 191us/step - loss: 0.1396 - acc: 0.9404 - val_loss: 4.7097 - val_acc: 0.4873\n",
            "Epoch 475/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1407 - acc: 0.9404 - val_loss: 4.4629 - val_acc: 0.4968\n",
            "Epoch 476/500\n",
            "470/470 [==============================] - 0s 171us/step - loss: 0.1408 - acc: 0.9362 - val_loss: 4.6972 - val_acc: 0.5032\n",
            "Epoch 477/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1391 - acc: 0.9404 - val_loss: 4.6379 - val_acc: 0.5096\n",
            "Epoch 478/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1409 - acc: 0.9404 - val_loss: 4.5043 - val_acc: 0.5032\n",
            "Epoch 479/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1386 - acc: 0.9404 - val_loss: 4.4643 - val_acc: 0.5064\n",
            "Epoch 480/500\n",
            "470/470 [==============================] - 0s 175us/step - loss: 0.1388 - acc: 0.9383 - val_loss: 4.4373 - val_acc: 0.5032\n",
            "Epoch 481/500\n",
            "470/470 [==============================] - 0s 165us/step - loss: 0.1419 - acc: 0.9404 - val_loss: 4.4191 - val_acc: 0.5127\n",
            "Epoch 482/500\n",
            "470/470 [==============================] - 0s 176us/step - loss: 0.1369 - acc: 0.9426 - val_loss: 4.3239 - val_acc: 0.5096\n",
            "Epoch 483/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1369 - acc: 0.9404 - val_loss: 4.3683 - val_acc: 0.5064\n",
            "Epoch 484/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1367 - acc: 0.9426 - val_loss: 4.2744 - val_acc: 0.5127\n",
            "Epoch 485/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1377 - acc: 0.9404 - val_loss: 4.0406 - val_acc: 0.5096\n",
            "Epoch 486/500\n",
            "470/470 [==============================] - 0s 190us/step - loss: 0.1413 - acc: 0.9404 - val_loss: 4.1247 - val_acc: 0.5064\n",
            "Epoch 487/500\n",
            "470/470 [==============================] - 0s 173us/step - loss: 0.1355 - acc: 0.9426 - val_loss: 4.4155 - val_acc: 0.5032\n",
            "Epoch 488/500\n",
            "470/470 [==============================] - 0s 172us/step - loss: 0.1429 - acc: 0.9426 - val_loss: 4.5999 - val_acc: 0.4904\n",
            "Epoch 489/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1426 - acc: 0.9383 - val_loss: 4.1825 - val_acc: 0.5064\n",
            "Epoch 490/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1462 - acc: 0.9404 - val_loss: 4.1018 - val_acc: 0.5127\n",
            "Epoch 491/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1401 - acc: 0.9383 - val_loss: 4.1088 - val_acc: 0.5096\n",
            "Epoch 492/500\n",
            "470/470 [==============================] - 0s 170us/step - loss: 0.1378 - acc: 0.9404 - val_loss: 4.0778 - val_acc: 0.5191\n",
            "Epoch 493/500\n",
            "470/470 [==============================] - 0s 168us/step - loss: 0.1379 - acc: 0.9404 - val_loss: 4.0493 - val_acc: 0.5159\n",
            "Epoch 494/500\n",
            "470/470 [==============================] - 0s 169us/step - loss: 0.1406 - acc: 0.9426 - val_loss: 3.9706 - val_acc: 0.5223\n",
            "Epoch 495/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1398 - acc: 0.9383 - val_loss: 4.1318 - val_acc: 0.5191\n",
            "Epoch 496/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1418 - acc: 0.9383 - val_loss: 4.3565 - val_acc: 0.5064\n",
            "Epoch 497/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1374 - acc: 0.9404 - val_loss: 4.3273 - val_acc: 0.5032\n",
            "Epoch 498/500\n",
            "470/470 [==============================] - 0s 167us/step - loss: 0.1369 - acc: 0.9383 - val_loss: 4.2651 - val_acc: 0.5159\n",
            "Epoch 499/500\n",
            "470/470 [==============================] - 0s 199us/step - loss: 0.1355 - acc: 0.9362 - val_loss: 4.2540 - val_acc: 0.5127\n",
            "Epoch 500/500\n",
            "470/470 [==============================] - 0s 164us/step - loss: 0.1411 - acc: 0.9404 - val_loss: 4.2710 - val_acc: 0.5159\n",
            "Test loss: 4.313788974558124\n",
            "Test accuracy: 0.5038167941206284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PwNx-5jJRglD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "8b4ee450-c519-485f-fb3e-d2216eeb12f0"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history_model_1.history['val_loss'])"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd1b0020f28>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdkAAAFKCAYAAABRtSXvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS4gsAACWEFkAACwhsgAAWPL/ADxmcMvz0OCUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd1b003ac50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QM7LdXljhhTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1871
        },
        "outputId": "ad2edb7a-0131-4c26-d45b-0eeff1d3987a"
      },
      "cell_type": "code",
      "source": [
        "history_model_1 = model_1.fit(X_train, y_train, validation_split=0.4, epochs=50, batch_size=128)\n",
        "score = model_1.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3364 samples, validate on 2243 samples\n",
            "Epoch 1/50\n",
            "3364/3364 [==============================] - 0s 144us/step - loss: 0.3748 - acc: 0.9194 - val_loss: 7.5266 - val_acc: 0.2657\n",
            "Epoch 2/50\n",
            "3364/3364 [==============================] - 0s 127us/step - loss: 0.3219 - acc: 0.9307 - val_loss: 7.6976 - val_acc: 0.2590\n",
            "Epoch 3/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 0.3094 - acc: 0.9313 - val_loss: 7.6302 - val_acc: 0.2679\n",
            "Epoch 4/50\n",
            "3364/3364 [==============================] - 0s 110us/step - loss: 0.2799 - acc: 0.9322 - val_loss: 7.6922 - val_acc: 0.2778\n",
            "Epoch 5/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 0.2532 - acc: 0.9402 - val_loss: 7.7405 - val_acc: 0.2639\n",
            "Epoch 6/50\n",
            "3364/3364 [==============================] - 0s 115us/step - loss: 0.2401 - acc: 0.9429 - val_loss: 7.6966 - val_acc: 0.2769\n",
            "Epoch 7/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 0.2291 - acc: 0.9423 - val_loss: 7.7754 - val_acc: 0.2639\n",
            "Epoch 8/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 0.2111 - acc: 0.9450 - val_loss: 7.9609 - val_acc: 0.2720\n",
            "Epoch 9/50\n",
            "3364/3364 [==============================] - 0s 117us/step - loss: 0.2054 - acc: 0.9444 - val_loss: 7.7059 - val_acc: 0.2786\n",
            "Epoch 10/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 0.1955 - acc: 0.9465 - val_loss: 7.8297 - val_acc: 0.2742\n",
            "Epoch 11/50\n",
            "3364/3364 [==============================] - 0s 117us/step - loss: 0.2019 - acc: 0.9441 - val_loss: 7.8874 - val_acc: 0.2755\n",
            "Epoch 12/50\n",
            "3364/3364 [==============================] - 0s 115us/step - loss: 0.1858 - acc: 0.9486 - val_loss: 7.8653 - val_acc: 0.2764\n",
            "Epoch 13/50\n",
            "3364/3364 [==============================] - 0s 120us/step - loss: 0.1833 - acc: 0.9486 - val_loss: 7.9129 - val_acc: 0.2751\n",
            "Epoch 14/50\n",
            "3364/3364 [==============================] - 0s 117us/step - loss: 0.1717 - acc: 0.9492 - val_loss: 7.9319 - val_acc: 0.2813\n",
            "Epoch 15/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 0.1719 - acc: 0.9471 - val_loss: 8.0400 - val_acc: 0.2617\n",
            "Epoch 16/50\n",
            "3364/3364 [==============================] - 0s 117us/step - loss: 0.1705 - acc: 0.9498 - val_loss: 8.1699 - val_acc: 0.2671\n",
            "Epoch 17/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 0.1605 - acc: 0.9512 - val_loss: 7.9204 - val_acc: 0.2791\n",
            "Epoch 18/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 0.1679 - acc: 0.9456 - val_loss: 7.9511 - val_acc: 0.2791\n",
            "Epoch 19/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 0.1555 - acc: 0.9521 - val_loss: 8.0165 - val_acc: 0.2835\n",
            "Epoch 20/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 0.1600 - acc: 0.9498 - val_loss: 8.0493 - val_acc: 0.2728\n",
            "Epoch 21/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 0.1540 - acc: 0.9536 - val_loss: 8.0386 - val_acc: 0.2760\n",
            "Epoch 22/50\n",
            "3364/3364 [==============================] - 0s 115us/step - loss: 0.1485 - acc: 0.9512 - val_loss: 7.9896 - val_acc: 0.2760\n",
            "Epoch 23/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 0.1445 - acc: 0.9527 - val_loss: 8.0710 - val_acc: 0.2844\n",
            "Epoch 24/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 0.1490 - acc: 0.9527 - val_loss: 8.0532 - val_acc: 0.2737\n",
            "Epoch 25/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 0.1435 - acc: 0.9515 - val_loss: 8.1907 - val_acc: 0.2764\n",
            "Epoch 26/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 0.1419 - acc: 0.9530 - val_loss: 8.0546 - val_acc: 0.2737\n",
            "Epoch 27/50\n",
            "3364/3364 [==============================] - 0s 114us/step - loss: 0.1440 - acc: 0.9527 - val_loss: 8.1391 - val_acc: 0.2827\n",
            "Epoch 28/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 0.1315 - acc: 0.9052 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 29/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 30/50\n",
            "3364/3364 [==============================] - 0s 117us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 31/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 32/50\n",
            "3364/3364 [==============================] - 0s 115us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 33/50\n",
            "3364/3364 [==============================] - 0s 115us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 34/50\n",
            "3364/3364 [==============================] - 0s 117us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 35/50\n",
            "3364/3364 [==============================] - 0s 116us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 36/50\n",
            "3364/3364 [==============================] - 0s 120us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 37/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 38/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 39/50\n",
            "3364/3364 [==============================] - 0s 120us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 40/50\n",
            "3364/3364 [==============================] - 0s 122us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 41/50\n",
            "3364/3364 [==============================] - 0s 126us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 42/50\n",
            "3364/3364 [==============================] - 0s 120us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 43/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 44/50\n",
            "3364/3364 [==============================] - 0s 122us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 45/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 46/50\n",
            "3364/3364 [==============================] - 0s 119us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 47/50\n",
            "3364/3364 [==============================] - 0s 121us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 48/50\n",
            "3364/3364 [==============================] - 0s 118us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 49/50\n",
            "3364/3364 [==============================] - 0s 120us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Epoch 50/50\n",
            "3364/3364 [==============================] - 0s 115us/step - loss: 1.1921e-07 - acc: 0.0000e+00 - val_loss: 1.1921e-07 - val_acc: 0.0000e+00\n",
            "Test loss: 1.1920930376163597e-07\n",
            "Test accuracy: 0.0002674511901577962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tpuMLcPCh0rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3b361e0f-ea1a-45cb-94a4-e4bc26d2dfb7"
      },
      "cell_type": "code",
      "source": [
        "example_product = 'The holidays are coming to LEGO City, one gift'\n",
        "example_product = preprocess(example_product)\n",
        "example_sequence = tokenizer.texts_to_sequences([example_product])\n",
        "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"-\"*10)\n",
        "print(\"Predicted category: \", category_reverse_index[model2.predict_classes(example_padded_sequence, verbose=0)[0]])\n",
        "print(\"-\"*10)\n",
        "probabilities = model2.predict(example_padded_sequence, verbose=0)\n",
        "probabilities = probabilities[0]\n"
      ],
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------\n",
            "Predicted category:  LEGO\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8UknHwZF3CV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1133
        },
        "outputId": "bb5e26b1-f1c4-4d5e-9176-6bb725c81f52"
      },
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "614     The holidays are coming to LEGO City, one gift...\n",
              "626     Product Description LEGO Friends Advent Calend...\n",
              "627     Product Description LEGO® Star Wars™ Jek-14’s ...\n",
              "634     Manufacturer's Description 24 days of LEGO® Ci...\n",
              "651     Product Description LEGO® Star Wars™ Corporate...\n",
              "720     Lego 7553 - City - Advent Calendar It's Christ...\n",
              "739     Celebrate 24 December days full of medieval bu...\n",
              "740         Lego Castle Advent Calendar 7979 NEW / SEALED\n",
              "748     Jedi Master Saesee Tiin is patrolling for Sepa...\n",
              "760     Product Description Count down to a LEGO Frien...\n",
              "765     Product Description Count down to a LEGO Frien...\n",
              "1142    LEGO Super Heroes: Plastic Man Set 5004081 (Ba...\n",
              "1167    Brand New Minifigure from LEGO. Includes Weapo...\n",
              "2395    Product Description The Malevolence is General...\n",
              "2397                     LEGO Racers 8384: Jungle Crasher\n",
              "2399    Product Description LEGO® Galaxy Squad Hive Cr...\n",
              "2405    Product Description Build a LEGO brick model o...\n",
              "2408       LEGO Indiana Jones: Colonel Dovchenko Keychain\n",
              "2409    Product Description LEGO Chima Nest Dive featu...\n",
              "2413    Product Description Construct a detailed LEGO ...\n",
              "2418    Product Description Unite with the Ice Bears w...\n",
              "2419               Lego Indiana Jones Shanghai Chase 7682\n",
              "2420    Product Description LEGO Creator Designer Set ...\n",
              "2422    Product Description The cute and playful pengu...\n",
              "2423                    LEGO Super Heroes: Flash Keychain\n",
              "2427    LEGO Legends of Chima: Crug's Swamp Jet Set 30...\n",
              "2428                     LEGO Star Wars: Watto Minifigure\n",
              "2432    Product Description LEGO Creator 4837: Mini Tr...\n",
              "2436             LEGO City: Lawn Mower Set 30224 (Bagged)\n",
              "2445    LEGO Classic Led Book Light, Blue, White and R...\n",
              "                              ...                        \n",
              "7827    Thomas & Friends - Track Master - Thomas - Bus...\n",
              "7909    Hot Wheels Gift Set (Set of 5 cars)Each pack c...\n",
              "7911    Box Contains 1 x Barbie doll with elbow pads, ...\n",
              "7934    Disney Princess Little Kingdom Magiclip Ariel'...\n",
              "7947    Disney Sleeping Beauty MagiClip Aurora Doll an...\n",
              "8460    Hot Wheels 2014 HW Race '13 FORD MUSTANG GT 16...\n",
              "8475                    new 75th Anniversary Batman cars!\n",
              "8509                    new 75th Anniversary Batman cars!\n",
              "8527    hot, wheels, hotwheels, dc, comics, universe f...\n",
              "8850    World Wrestling Entertainment Elite Collection...\n",
              "8868    Product Description Jake's musical pirate ship...\n",
              "8921                                            BRAND NEW\n",
              "8944    Hook Jake & Skully - Play with your favourite ...\n",
              "8952    DC Universe Classics Wave 12 > Spectre Action ...\n",
              "9005    Celebrating Batman video games and classic DC ...\n",
              "9008    Product Description Collect your favourite Oct...\n",
              "9024                    new 75th Anniversary Batman cars!\n",
              "9027        Toy Story Rocket Running Buzz Lightyear [Toy]\n",
              "9137                                                  New\n",
              "9164    When kids aspire to be like Batman, it's easy ...\n",
              "9169    Scorpia Masters of the Universe Classics Actio...\n",
              "9237    Manufacturer's Description Extend the fun of y...\n",
              "9306    Product Description Ever dreamed of moving an ...\n",
              "9420    From the Manufacturer This infamous wild card ...\n",
              "9449    Product Description I have mixed the bar-codes...\n",
              "9539    Product Description Description Product Descri...\n",
              "9898    Product Description Be the first to answer Fri...\n",
              "9915    Product Description Major new license game for...\n",
              "9933      Scene it? Magical Moments Disney DVD Game [Toy]\n",
              "9995    DC 66 Batman Classic TV Series 6 Inch Riddler ...\n",
              "Name: description, Length: 1046, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 376
        }
      ]
    },
    {
      "metadata": {
        "id": "kfZhXl8JISdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "c24df0a0-7bb1-455a-f226-1a97b2597fa3"
      },
      "cell_type": "code",
      "source": [
        "# Wikipedia 2014\n",
        "! wget 'http://nlp.stanford.edu/data/glove.6B.zip'"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-13 17:32:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-11-13 17:32:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.57MB/s    in 8m 9s   \n",
            "\n",
            "2018-11-13 17:40:40 (1.68 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e0rHiX6BL0_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a3af2fe3-e654-4f3a-f071-6e11e4220e5d"
      },
      "cell_type": "code",
      "source": [
        "! unzip glove.6B.zip"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TIVue_Gq9DI4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
      ]
    }
  ]
}